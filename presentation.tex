\documentclass{article}
\setlength{\textheight}{9.3in}
\setlength{\textwidth}{6.68in}
\setlength{\oddsidemargin}{-.20in}
\setlength{\topmargin}{-.25in}
\setlength{\headsep}{10mm}
\pagestyle{myheadings}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{multicol}
\title{\hrule \vspace{0.3cm}MLE from a Competing Risks Model of an Exponential Failure and a Lognormal Survival}
\date{10 November 2010}
\author{Nikola Chochkov, MSc Statistics, Humboldt University Berlin}
\begin{document}
\maketitle
\hrule
\section{Introduction.}
\subsection{Assumptions.}
\indent \indent Two usage-measured lifetime random variables are discussed in a fixed-time life test. The items under study are considered to accumulate usage independently and with a different rate. The survival ssspopulation is considered \textit{Lognormally distributed} (with parameters $\mu$ and $\sigma$), while the failure popoulation - \textit{Exponential} (with parameter $\lambda$). 
\\ 
\\ \indent An estimation of the parameter vector ($\lambda$, $\mu$, $\sigma$)$'$ is sought using all usage data collected (from both survival and failure cases) 
\begin{itemize}
\item $\eta \sim Lognormal$ 
\item $\psi \sim Exponential$ 
\item $f_\eta(x) = \frac{1}{x \sigma \sqrt{2 \pi}} e^{-\frac{\left( \ln x - \mu \right)^2}{2\sigma^2}} = \frac{1}{x \sigma} \phi \left( \frac{\ln x - \mu}{\sigma} \right) $ is the \textit{probability densitiy function} of the Lognormal distribution, with $\phi$ being the density function of the Standard Normal Distribution
\item $\overline F_\eta(x) = 1 - \frac{1}{\sqrt{2 \pi}} e^{-\frac{\left( \ln x - \mu \right)^2}{2\sigma^2}} = \overline \Phi \left(\frac{\ln x - \mu}{\sigma}\right)$ is its \textit{survival function}, with $\overline\Phi$ being the survival function of the Standard Normal Distribution
\item $f_\psi(x) = \lambda e^{- \lambda x}$ is the \textit{pdf} of the Exponential distribution 
\item $\overline F_\psi(x) = e^{- \lambda x}$ is its \textit{survival function}
\item $N_f = \{i : x_i\ is\ failed \}$ and $n_f  = \# N_f$ (i.e. number of failures) 
\item $N_s = \{i : x_i\ is\ survived \}$ and $n_f  = \# N_s$ (i.e. number of survivals)
\item $n$ is the total number of units under test ($n = n_f + n_s$)
\item $\textbf{x}$ is the data vector and $\textbf{x} = \left[ \textbf{x}^f, \textbf{x}^s \right] = \left[ x_1^f, ... , x_{n_f}^f, x_1^s, ... , x_{n_s}^s \right] $, where:
\item $\textbf{x}^f$ is the data for failed items, $\textbf{x}^f = \left[ x_1^f, ... , x_{n_f}^f \right] $
\item $\textbf{x}^s$ is the data for survived items, $\textbf{x}^s = \left[ x_1^s, ... , x_{n_s}^s \right] $
\item $\lambda, \mu, \sigma, x > 0$
\end{itemize}
\subsection{Maximum Likelihood Method.}
\indent \indent The Likelihood function is given by (since $\eta$ and $\psi$ are independent):
\begin{eqnarray}
L(\lambda, \mu, \sigma | \textbf{x}) &=& \prod_{i \in N_f} \left[ f_\psi \left( x_i^f \right) \overline F_\eta \left( x_i^f \right) \right]\prod_{i \in N_s} \left[ f_\eta \left( x_i^s \right) \overline F_\psi \left( x_i^s \right) \right]
\end{eqnarray}
\indent Now from the above we can derive the Log Likelihood:
\begin{eqnarray}
LogL = L^*(\lambda, \mu, \sigma | \textbf{x}) &=& \sum_{i \in N_f} \ln \left[ f_\psi(x_i^f) \right] + \sum_{i \in N_f} \ln \left[ \overline F_\eta(x_i^f) \right] + \sum_{i \in N_s} \ln \left[ f_\eta (x_i^s) \right] + \sum_{i \in N_s} \ln \left[ \overline F_\psi(x_i^s) \right]
\end{eqnarray}
\indent And if we apply the above notation and rework:
\begin{eqnarray}
L^*(\lambda, \mu, \sigma | \textbf{x}) &=& N_f \ln \lambda - \lambda \sum_{i = 1}^n x_i + \sum_{i \in N_f} \ln \overline \Phi \left( \frac{\ln x_i^f - \mu}{\sigma} \right) - \sum_{i \in N_s} \ln \sigma x_i^s + \sum_{i \in N_s} \ln \phi \left( \frac{\ln x_i^s - \mu}{\sigma} \right)
\end{eqnarray}
\indent The MLE estimators $\left(\hat \lambda, \hat \mu, \hat \sigma \right)$ would be the ones that turn $L^*$ into a minimum, so now we need to compute the \textit{Gradient} vector and \textit{Hessian} matrix in order to find them. Furthermore we would like to show that the estimators have the \textit{consistency} property.\\ 
\subsection{First order derivatives of $L^*$ w.r.t $\left(\lambda, \mu, \sigma \right)$}
\indent Let's denote: $z_i^{f,s} = \frac{\left(\ln x_i^{f,s} - \mu \right)}{\sigma}$. Then we derive: 
\begin{eqnarray}
\frac{\partial z}{\partial \mu} = - \frac{1}{\sigma} , \frac{\partial z} {\partial \sigma} = - \frac{1}{\sigma}z , \frac{\partial \phi}{\partial \sigma} = \frac{1}{\sigma}\phi z^2 , \frac{\partial \phi}{\partial \mu} = \frac{1}{\sigma}\phi z 
\end{eqnarray} 
\begin{eqnarray}
\frac{\partial L^*(\lambda, \mu, \sigma | \textbf{x}) }{\partial \lambda} &=& \frac{n_f}{\lambda} - \sum_{i = 1}^n x_i 
\end{eqnarray} 
\begin{eqnarray}
\frac{\partial L^*(\lambda, \mu, \sigma | \textbf{x}) }{\partial \mu} &=& \frac{1}{\sigma}\sum_{i \in N_f} \frac{\phi \left( z_i^f \right)}{\overline \Phi \left( z_i^f \right)} + \frac{1}{\sigma}\sum_{i \in N_s}z_i^s 
\end{eqnarray} 
\begin{eqnarray}
\frac{\partial L^*(\lambda, \mu, \sigma | \textbf{x}) }{\partial \sigma} &=& \frac{1}{\sigma} \sum_{i \in N_f} \frac{\phi \left( z_i^f \right) z_i^f }{\overline \Phi \left( z_i^f \right)} + \frac{1}{\sigma}\sum_{i \in N_s} (z_i^s)^2 - \frac{n_s}{\sigma} 
\end{eqnarray} 
\subsection{Second order derivatives of $L^*$ w.r.t $\left(\lambda, \mu, \sigma \right)$}\
\indent Let's denote again: $z_i^{f,s} = \frac{\left(\ln x_i^{f,s} - \mu \right)}{\sigma}$. Then we derive: 
\begin{eqnarray}
\frac{\partial^2 L^*(\lambda, \mu, \sigma | \textbf{x}) }{\partial \lambda^2} &=& - \frac{n_f}{\lambda ^ 2} 
\end{eqnarray} 
\begin{eqnarray}
\frac{\partial^2 L^*(\lambda, \mu, \sigma | \textbf{x}) }{\partial \mu^2} &=& \frac{1}{\sigma^2}\sum_{i \in N_f} \left[ \frac{\phi \left( z_i^f \right)}{\overline \Phi \left( z_i^f \right)}\left( 1 + \frac{\phi \left( z_i^f \right)}{\overline \Phi \left( z_i^f \right)} \right) z_i^f \right] - \frac{1}{\sigma^2} 
\end{eqnarray} 
\begin{eqnarray}
\frac{\partial^2 L^*(\lambda, \mu, \sigma | \textbf{x}) }{\partial \mu \partial\sigma} &=& \frac{1}{\sigma^2} \sum_{i \in N_f} \frac{\phi \left( z_i^f \right)}{\overline \Phi \left( z_i^f \right)} + \frac{1}{\sigma^2}\sum_{i \in N_f} \left[ \frac{\phi \left( z_i^f \right)}{\overline \Phi \left( z_i^f \right)}\left( 1 + \frac{\phi \left( z_i^f \right)}{\overline \Phi \left( z_i^f \right)} \right) z_i^f \right] - \frac{2}{\sigma^2}\sum_{i \in N_s} z_i^s 
\end{eqnarray} 
\begin{eqnarray}
\frac{\partial^2 L^*(\lambda, \mu, \sigma | \textbf{x}) }{\partial \sigma^2} &=& \frac{1}{\sigma^2} \sum_{i \in N_f} \left[ \frac{\phi (z_i) z_i^3}{\overline \Phi} - 2\frac{\phi(z_i)z_i}{\overline \Phi} - \frac{\phi^2(z_i) z_i^2}{\overline\Phi ^2} \right] - \frac{3}{\sigma^2}\sum_{i \in N_s} z_i^2 + \frac{n_s}{\sigma^2}
\end{eqnarray} 
\begin{itemize}
\item \textit{Theoretical Variance/Covariance Matrix} - this is the Inverse of the negative Hessian of the Log-Likelihood, taken at the true parameter values. I found and ignored some \textit{data sets} that produce negative variance or larger than one correlation between parameters;
\item \textit{Eigen values of Hessian matrix} - I found and ignored some \textit{data sets} that produce non-negative definite Hessian;
\item \textit{Convergence of Optimization Iterations} - I ignored \textit{data sets}, from which the \textit{Newton-Raphson-like} iterative method I use - \textit{nlminb()} - did not converge. In case convergence is not achieved for some data set, we could attempt to change the initial point for iterations, so ignoring these cases would not lead to loss of generality;
\end{itemize}
\section{Maximum Likelihood Estimators' properties.}
\subsection{Mean of Score.} Testing the estimates and the data I collected about the $Log L^*$ during the simulations, I tested the average of the \textit{Gradient} elements taken at true parameter values (aka as Score). A well known property of the Maximum Likelihood Estimator is that the asymptotic average of the Score tends to zero. The simple proof comes below ($\Theta$ is the vector of parameters):\\
\begin{eqnarray}
E(\frac{\partial log L^*(\Theta | x)}{\partial \Theta}) = \int \frac{\partial L^*(\Theta | x)/\partial \Theta}{L^*(\Theta | x)}L^*(\Theta | x)\partial x = \int \frac{\partial L^*(\Theta | x)}{\partial \Theta} \partial x = \frac{\partial}{\partial \Theta} \int L^*(\Theta | x) \partial x 
\end{eqnarray} 
For the last equality to hold, however, regularity conditions need to be met. Specifically, the function $L^*(\Theta | x)$ and its first partial derivatives have to be continuous in all arguments.\\ 
\\Table 1. shows the simulation details and histograms of the Score functions taken at the true parameter values. 
\section{Full Likelihood.}
According to the histograms on Table 1, the condition (2) holds only for parameter $\lambda$, whereas the scores with respect to $\mu$ and $\sigma$ do not tend to average at zero.\\
\\To confirm this, I constructed a different likelihood based also on the generated data for censored mileages. The so constructed likelihood is a \textit{Full Likelihood} in the sense that every observation contributes to it and there's \textit{no unknown mileage}.   
\begin{eqnarray}
L(\lambda, \mu, \sigma, \eta, m, \textbf{x}, \textbf{y}) &=& \prod_{t=1}^{T} \left[ \prod_{i=1}^{r_t^{(a)}} \left( f_a(x_i^{(a)}) \bar{F_b}(x_i^{(a)}) \bar{G}(x_i^{(a)}) \right) \prod_{i=1}^{r_t^{(b)}} \left( f_b(x_i^{(b)}) \bar{F_a}(x_i^{(b)}) \bar{G}(x_i^{(b)}) \right) \prod_{i=i}^{c_t} \left( g(y_i)\bar{F_a}(y_i)\bar{F_b}(y_i) \right) \right] 
\end{eqnarray}
I apply the same approach as above to estimate all \textit{five} parameters simultaneously. I derived the Gradient and Hessian functions in this case and performed the same simulation - now with \textit{no practical use}, but only useful to compare my estimations. \\ 
\section{Conclusion. } As expected, the results in the Full Likelihood case seem very good and seem to possess the properties of MLE; Table 2 shows them. However, the same trend is noticed with respect to the condition (2) - it holds for the \textit{Exponential} and \textit{Weibull} parameters, but does not seem to hold for the \textit {Log-normal} parameters.\\
\\ Furthermore, the estimates for $\mu$ and $\sigma$ appear not to conform with another MLE property - the one for normality around the true value. Tables 2 shows the results for one parameter set from Likelihood (3) as well as the histograms of the Score functions. The same trends I obtained in other parameter sets as well.\\
\\ These results make me doubt that the functions $L^*$ and $L$ or their first-order derivatives might be discontinuous with respect to parameters $\mu$ and $\sigma$. 
\section{Next steps.}
I hope to check whether the departure from zero of the mean of the Score w.r.t. $\mu$ and $\sigma$ is significant. If so then, is it due to discontinuousness of the likelihood or their derivatives. At the same time I continue to try to find errors in my simulation code or calculations, in which I have so far been unsuccessful.
\end{document}

